{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b0f8a0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "The <a href=\"https://www.cato.org/human-freedom-index/2021 \">Human Freedom Index</a> measures economic freedoms such as the freedom to trade or to use sound money, and it captures the degree to which people are free to enjoy the major freedoms often referred to as civil liberties—freedom of speech, religion, association, and assembly— in the countries in the survey. In addition, it includes indicators on rule of law, crime and violence, freedom of movement, and legal discrimination against same-sex relationships. We also include nine variables pertaining to women-specific freedoms that are found in various categories of the index.\n",
    "\n",
    "<u>Citation</u>\n",
    "\n",
    "Ian Vásquez, Fred McMahon, Ryan Murphy, and Guillermina Sutter Schneider, The Human Freedom Index 2021: A Global Measurement of Personal, Civil, and Economic Freedom (Washington: Cato Institute and the Fraser Institute, 2021).\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4aa5e5",
   "metadata": {},
   "source": [
    "## Using simple MLP CLassifier from sklearn for the first part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e8b8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9295578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"https://github.com/jnin/information-systems/raw/main/data/hfi_cc_2021.csv\")\n",
    "df.drop([\"year\", \"ISO\", \"countries\"], axis = 1, inplace = True)\n",
    "\n",
    "columns_rank = list(columns for columns in df if \"rank\" in columns)\n",
    "df.drop(columns_rank, axis = 1, inplace = True)\n",
    "\n",
    "columns_score = list(columns for columns in df if \"score\" in columns)\n",
    "df.drop(columns_score, axis = 1, inplace = True)\n",
    "\n",
    "df.dropna(subset = [\"hf_quartile\"], inplace = True)\n",
    "\n",
    "X = df.drop([\"hf_quartile\"], axis = 1)\n",
    "y = df[[\"hf_quartile\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcdf3d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "categorical_values = X_train.select_dtypes(include = [\"object\"]).columns.tolist()\n",
    "\n",
    "numerical_values = X_train.select_dtypes(exclude = [\"object\"]).columns.tolist()\n",
    "\n",
    "transformer = ColumnTransformer([(\"encode\", OneHotEncoder(),categorical_values)], remainder = \"passthrough\")\n",
    "\n",
    "pipe = Pipeline([(\"encoder\", transformer), \n",
    "                 (\"imputer\", SimpleImputer(strategy = \"most_frequent\")), \n",
    "                 (\"scaler\", StandardScaler()), \n",
    "                 (\"neural_model\", MLPClassifier(max_iter = 250))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0836966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"neural_model__learning_rate_init\" : [0.001, 0.0001],\n",
    "    \"neural_model__alpha\" : [0.0001, 1]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv = 3)\n",
    "\n",
    "grid.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "training_score = grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bb237be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  {0.9357172834854319}\n",
      "Test score:  {0.9571734475374732}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_model = grid.best_estimator_.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "score = best_model.score(X_test, y_test)\n",
    "\n",
    "# Comparing the training and test scores to check for overfitting\n",
    "print('Training score: ',{training_score})\n",
    "print('Test score: ', {score}) \n",
    "\n",
    "# It does not seem like the model is overfitting as the training and test scores are very similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a3298",
   "metadata": {},
   "source": [
    "## Using Keras Neural Networks to create a Sequential model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4066abff",
   "metadata": {},
   "source": [
    "#### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee35629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.decomposition import PCA\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.regularizers import l2, l1\n",
    "from keras_tuner import HyperParameters\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import LeakyReLU\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526ee79",
   "metadata": {},
   "source": [
    "#### Preparing the dataset\n",
    "\n",
    "\n",
    "*   Loading the dataset into a dataframe and dropping irrelevant columns\n",
    "*   Removing rows where the target variable (hf_quartile) is missing\n",
    "*   Splitting the dataframe into independent and dependent (target) variables\n",
    "*   Splitting the datasets into training and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3767d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Loading the dataset and dropping irrelevant columns, and columns with rank or score \n",
    "df = pd.read_csv(\"https://github.com/jnin/information-systems/raw/main/data/hfi_cc_2021.csv\")\n",
    "df.drop([\"year\", \"ISO\", \"countries\"], axis = 1, inplace = True)\n",
    "\n",
    "columns_rank = list(columns for columns in df if \"rank\" in columns)\n",
    "df.drop(columns_rank, axis = 1, inplace = True)\n",
    "\n",
    "columns_score = list(columns for columns in df if \"score\" in columns)\n",
    "df.drop(columns_score, axis = 1, inplace = True)\n",
    "\n",
    "# Dropping rows in which the target variable value is missing\n",
    "df.dropna(subset = [\"hf_quartile\"], inplace = True)\n",
    "\n",
    "# Separating the independent and dependent (target) variables\n",
    "X = df.drop([\"hf_quartile\"], axis = 1)\n",
    "y = df[[\"hf_quartile\"]]\n",
    "\n",
    "# Splitting the dataset into train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.80, random_state = 123) \n",
    "# Different random_state / seeds and a different train / test split sizes were tested to check if they improve the model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b4d399",
   "metadata": {},
   "source": [
    "#### **Defining a neural network model using Keras library with 4 fully connected layers**\n",
    "\n",
    "*   **Layer 1:** 50 neurons, LeakyReLU activation, and L2 regularization\n",
    "*   **Layers 2:** 30 neurons, LeakyReLU activation and L2 regularization\n",
    "*   **Layer 3:** 10 neurons, LeakyReLU activation and L2 regularization\n",
    "*   **Layer 4:** 4 units and a softmax activation function\n",
    "\n",
    "**Notes about the network architecture:**\n",
    "\n",
    "- We start with 50 neurons in the first layer and decrease with each layer to gradually reduce the complexity of the model and prevent overfitting. We experimented with different sizes such as starting with 100 and decreasing by 20 per layer or starting with 200 and decreasing by 50 per layer and checked accuracy / overfitting with each combination until we finally decided on these sizes through trial and error.\n",
    "\n",
    "- LeakyReLU is the activation function used in layers 1-3  as it is known to perform well in deep neural networks. We tried increasing the alpha from the common practice of 0.1 to 0.2 to increase regularization (and therefore reduce chances of overfitting).\n",
    "\n",
    "- L2 regularization is used in layers 1-3 to prevent overfitting by adding a penalty term to the loss function that encourages the model to have smaller weights.\n",
    "\n",
    "- Dropout layers are used as an additional regularization technique that randomly drops out some of the neurons in the network during training, which helps further prevent overfitting. Here, a dropout rate of 20% is used. We also experimented with this parameter starting with 10% then increasing it to reduce overfitting.\n",
    "\n",
    "- Softmax is the activation function used in the final layer because it can map the model's final outputs to probabilities that sum to 1 making it ideal for multi-class classification problems such as this one.\n",
    "\n",
    "- The model is compiled using the Adam optimizer with a learning rate of 0.001, sparse categorical crossentropy loss, and accuracy as the evaluation metric. This neural network model is designed for multi-class classification problems, where there are four possible output classes (in this case a Human Freedom Quartile of 1, 2, 3 or 4). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08779cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Sequential model object\n",
    "model = Sequential()\n",
    "\n",
    "## Layer 1\n",
    "# Adding a fully connected layer with 100 units, LeakyReLU activation with alpha value of 0.1, and L2 regularization with a strength of 0.001\n",
    "# The input dimension of the layer is 122 to match the number of featurs in the dataset\n",
    "model.add(Dense(units = 50, activation = LeakyReLU(alpha = 0.2), input_dim = (122), kernel_regularizer = l2(0.001)))\n",
    "\n",
    "# Adding a dropout layer with 10% dropout rate to avoid overfitting\n",
    "model.add(Dropout(0.2)) \n",
    "\n",
    "\n",
    "## Layer 2\n",
    "# Adding another fully connected layer with 80 units, LeakyReLU activation with alpha value of 0.1, and L2 regularization with a strength of 0.001\n",
    "model.add(Dense(units = 30, activation = LeakyReLU(alpha = 0.2), kernel_regularizer = l2(0.001)))\n",
    "\n",
    "# Adding a dropout layer with 10% dropout rate to avoid overfitting\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "## Layer 3\n",
    "# Adding another fully connected layer with 60 units, LeakyReLU activation with alpha value of 0.1, and L2 regularization with a strength of 0.001\n",
    "model.add(Dense(units = 10, activation = LeakyReLU(alpha = 0.2), kernel_regularizer = l2(0.001)))\n",
    "\n",
    "# Adding a dropout layer with 10% dropout rate to avoid overfitting\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "## Layer 4\n",
    "# Adding a final fully connected layer with 4 units and softmax activation\n",
    "# This layer outputs probabilities for each of the 4 output classes\n",
    "model.add(Dense(units = 4, activation = \"softmax\"))\n",
    "\n",
    "# Compiling the model with Adam optimizer with a learning rate of 0.001, sparse categorical crossentropy loss, and accuracy as the evaluation metrica\n",
    "model.compile(optimizer = Adam(learning_rate = 0.001), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5475ddb2",
   "metadata": {},
   "source": [
    "#### Creating a Pipeline consisting of a SimpleImputer with the most frequent strategy, a OneHotEncoder for the categorical variables, a standard scaler, and a KerasClassifier model specifying 65 epochs and batch sizes of 100.\n",
    "\n",
    "The number of epochs and batch sizes were decided through trial and error of different values until the best balance was reach between accuracy, (lack of) overfitting and compute time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc7e59c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the categorical features to be encoded\n",
    "categorical_values = X_train.select_dtypes(include = [\"object\"]).columns.tolist()\n",
    "\n",
    "numerical_values = X_train.select_dtypes(exclude = [\"object\"]).columns.tolist()\n",
    "\n",
    "# Applying one-hot encoding to categorical features\n",
    "transformer = ColumnTransformer([(\"encode\", OneHotEncoder(),categorical_values)], remainder = \"passthrough\")\n",
    "\n",
    "# Applying the early stopping technique to stop training if there is no improvement in loss for 10 epochs, and restore_best_weights=True to restore the best model weights\n",
    "early_stop = EarlyStopping(monitor = 'loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Defining the steps for preprocessing the data and training the neural network with scikit-learn and Keras\n",
    "pipe2 = Pipeline([(\"encoder\", transformer), \n",
    "                 (\"imputer\", SimpleImputer(strategy = \"most_frequent\")), \n",
    "                 (\"scaler\", StandardScaler()),\n",
    "                 (\"model\", KerasClassifier(build_fn = model, epochs = 65, batch_size = 100, callbacks=[early_stop]))])\n",
    "# Specifying the training parameters (batch sizes and number of epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682fa097",
   "metadata": {},
   "source": [
    "### Fitting the model using the created pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec150f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\scikeras\\wrappers.py:301: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 2ms/step - loss: 1.5277 - accuracy: 0.3831\n",
      "Epoch 2/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 1.1397 - accuracy: 0.5405\n",
      "Epoch 3/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.9396 - accuracy: 0.6417\n",
      "Epoch 4/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.8792 - accuracy: 0.6758\n",
      "Epoch 5/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.8025 - accuracy: 0.7200\n",
      "Epoch 6/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.7291 - accuracy: 0.7488\n",
      "Epoch 7/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6590 - accuracy: 0.7904\n",
      "Epoch 8/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.6345 - accuracy: 0.7944\n",
      "Epoch 9/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5718 - accuracy: 0.8265\n",
      "Epoch 10/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5673 - accuracy: 0.8131\n",
      "Epoch 11/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5140 - accuracy: 0.8480\n",
      "Epoch 12/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4972 - accuracy: 0.8573\n",
      "Epoch 13/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.8593\n",
      "Epoch 14/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4532 - accuracy: 0.8828\n",
      "Epoch 15/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4448 - accuracy: 0.8821\n",
      "Epoch 16/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4316 - accuracy: 0.8855\n",
      "Epoch 17/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4185 - accuracy: 0.8855\n",
      "Epoch 18/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3881 - accuracy: 0.8962\n",
      "Epoch 19/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3672 - accuracy: 0.9123\n",
      "Epoch 20/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3774 - accuracy: 0.9082\n",
      "Epoch 21/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3481 - accuracy: 0.9183\n",
      "Epoch 22/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3519 - accuracy: 0.9216\n",
      "Epoch 23/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3538 - accuracy: 0.9203\n",
      "Epoch 24/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3407 - accuracy: 0.9196\n",
      "Epoch 25/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3325 - accuracy: 0.9196\n",
      "Epoch 26/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3459 - accuracy: 0.9210\n",
      "Epoch 27/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3357 - accuracy: 0.9183\n",
      "Epoch 28/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3178 - accuracy: 0.9290\n",
      "Epoch 29/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3152 - accuracy: 0.9270\n",
      "Epoch 30/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3293 - accuracy: 0.9310\n",
      "Epoch 31/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3050 - accuracy: 0.9324\n",
      "Epoch 32/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3124 - accuracy: 0.9297\n",
      "Epoch 33/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3182 - accuracy: 0.9303\n",
      "Epoch 34/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2941 - accuracy: 0.9364\n",
      "Epoch 35/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2812 - accuracy: 0.9404\n",
      "Epoch 36/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2817 - accuracy: 0.9411\n",
      "Epoch 37/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2780 - accuracy: 0.9444\n",
      "Epoch 38/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2787 - accuracy: 0.9377\n",
      "Epoch 39/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2720 - accuracy: 0.9384\n",
      "Epoch 40/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2703 - accuracy: 0.9484\n",
      "Epoch 41/65\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2789 - accuracy: 0.9357\n",
      "Epoch 42/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2613 - accuracy: 0.9498\n",
      "Epoch 43/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2634 - accuracy: 0.9498\n",
      "Epoch 44/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2599 - accuracy: 0.9451\n",
      "Epoch 45/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2654 - accuracy: 0.9424\n",
      "Epoch 46/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2698 - accuracy: 0.9417\n",
      "Epoch 47/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2507 - accuracy: 0.9484\n",
      "Epoch 48/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2371 - accuracy: 0.9538\n",
      "Epoch 49/65\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2399 - accuracy: 0.9471\n",
      "Epoch 50/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2424 - accuracy: 0.9524\n",
      "Epoch 51/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2418 - accuracy: 0.9504\n",
      "Epoch 52/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2235 - accuracy: 0.9591\n",
      "Epoch 53/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2215 - accuracy: 0.9591\n",
      "Epoch 54/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2180 - accuracy: 0.9632\n",
      "Epoch 55/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2263 - accuracy: 0.9605\n",
      "Epoch 56/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2244 - accuracy: 0.9598\n",
      "Epoch 57/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2300 - accuracy: 0.9551\n",
      "Epoch 58/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2226 - accuracy: 0.9585\n",
      "Epoch 59/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2259 - accuracy: 0.9625\n",
      "Epoch 60/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2059 - accuracy: 0.9678\n",
      "Epoch 61/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2224 - accuracy: 0.9538\n",
      "Epoch 62/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2280 - accuracy: 0.9551\n",
      "Epoch 63/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2195 - accuracy: 0.9565\n",
      "Epoch 64/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2188 - accuracy: 0.9612\n",
      "Epoch 65/65\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2125 - accuracy: 0.9578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('encoder',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('encode', OneHotEncoder(),\n",
       "                                                  ['region'])])),\n",
       "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('model',\n",
       "                 KerasClassifier(batch_size=100, build_fn=<keras.engine.sequential.Sequential object at 0x00000171FBFDFE20>, callbacks=[<keras.callbacks.EarlyStopping object at 0x00000171FC16D730>], epochs=65))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5482ad",
   "metadata": {},
   "source": [
    "#### Checking the accuracy score\n",
    "The best achieved score was 95.99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "065a3200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n",
      "0.9598930481283422\n"
     ]
    }
   ],
   "source": [
    "final_score = pipe2.score(X_test, y_test)\n",
    "print(final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "198141bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 719us/step\n",
      "Training score:  0.9866041527126591\n",
      "Test score:  {0.9598930481283422}\n"
     ]
    }
   ],
   "source": [
    "# Comparing the training and test scores to check for overfitting\n",
    "print('Training score: ',pipe2.score(X_train, y_train))\n",
    "print('Test score: ', {final_score}) \n",
    "\n",
    "# It does not seem like the model is overfitting as the training and test scores are similar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
